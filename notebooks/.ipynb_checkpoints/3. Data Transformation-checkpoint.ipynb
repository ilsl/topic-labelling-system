{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Transformation (and futher cleaning)\n",
    "#### Written by: Isobel Jones\n",
    "#### Date: 14th of June\n",
    "\n",
    "1. Read in cleaned data\n",
    "2. Convert data in a tuple\n",
    "3. Preprocess tweets\n",
    "    - Lemmitise\n",
    "    - Normalise\n",
    "    - Remove Stop words\n",
    "    - Remove hashtags\n",
    "    - Remove RT and cc\n",
    "    - Remove URLs\n",
    "    - Remove mentions (GDPR compliant, this information also doesn’t add value to build sentiment analysis model.)\n",
    "    - Remove punctuation\n",
    "    - Tokenize text\n",
    "    - Convert text to lower case\n",
    "4. Write preprocessed tuples to serialized pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "import pickle\n",
    "import timeit\n",
    "import tokenizer\n",
    "import collections\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isobeljones/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/isobeljones/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read in cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/isobeljones/DataScience/automating-social-media\n"
     ]
    }
   ],
   "source": [
    "##tweetxamp needs to be each individual tweet\n",
    "# for x in range(0,len(Royalmaillist_df)-1):\n",
    "#     for tag in listnames:\n",
    "#        Royalmaillist_df.Tweet[x] = Royalmaillist_df.Tweet[x].replace(tag, gocept.pseudonymize.name(tag,'secret'))\n",
    "# print (Royalmaillist_df.Tweet[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        next(reader)\n",
    "        for line in reader:\n",
    "            (Id, Tweet, Label) = parseTweet(line)\n",
    "            rawData.append((Id, Tweet, Label))\n",
    "            preprocessedData.append((Id, preProcess(Tweet), Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['something', 'is', 'happening', 'here']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correctspelling(listofwords):\n",
    "    correctspellinglist=[]\n",
    "    spell = SpellChecker() \n",
    "    for word in listofwords:\n",
    "        # Get the one `most likely` answer\n",
    "        if word in spell.unknown(listofwords):\n",
    "            correctspellinglist.append(spell.correction(word))\n",
    "        else:\n",
    "            correctspellinglist.append(word)\n",
    "    return correctspellinglist\n",
    "misspelled = ['something', 'is', 'hapenning', 'here']\n",
    "correctspelling(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/tweet/label tuple\n",
    "\n",
    "def parseTweet(tweetLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    \n",
    "    #Get the first column of data and convert to datatype int\n",
    "    id = int(tweetLine[0])\n",
    "    \n",
    "    #Get the second column of data and convert to datatype string\n",
    "    text = ''.join(tweetLine[1:2])\n",
    "    \n",
    "    #Get the third column of data and convert to datatype string\n",
    "    label = ''.join(tweetLine[2:3])\n",
    "\n",
    "    return (id , text, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsmodified = ['i',\n",
    "# List of step words that I have identified through deep dive of end results                     \n",
    " 'get','when','day', 'royal','get','doe','dm','say','u','amp','next','still','pm',\n",
    " 'me','item','how','not','hi','still','got','th','letter','wa','may','po','need',\n",
    " 'st','today','hear','check','could','update','even','sent','much','already','please',\n",
    " 'help','people','saying','go','x','one','since',\n",
    " 'one','somenone','way','local','raised','hour','get','yet','back','said','want','week'\n",
    "\n",
    "# List of of generics out of the box stop words                    \n",
    " 'put','know','would','think','ok','ago','getting','think','back'\n",
    " 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \n",
    " \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', \n",
    " 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", \n",
    " 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', \n",
    " 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was',\n",
    " 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', \n",
    " 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while',\n",
    " 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', \n",
    " 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out',\n",
    " 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
    "  'why',  'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some',\n",
    " 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', \n",
    " 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    " 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\",  'doesn', \"doesn't\", 'hadn', \"hadn't\", \n",
    "  'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\",\n",
    " 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \n",
    " 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing and feature vectorization\n",
    "stopwordscached = stopwords.words('english')\n",
    "\n",
    "# Input: a string of one review\n",
    "\n",
    "def preProcess(tweet):\n",
    "    \n",
    "    #Clean tweets\n",
    "    tweet = re.sub('http\\S+\\s*', '', tweet)  # remove URLs\n",
    "    tweet = re.sub('RT|cc', '', tweet)  # remove RT and cc\n",
    "    \n",
    "    #Add spaces inbetween punctuation\n",
    "    tweet = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", tweet)\n",
    "    tweet = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", tweet)\n",
    "    \n",
    "    # normalisation\n",
    "    tweet = re.sub(r\"(\\S)\\1\\1+\",r\"\\1\\1\\1\", tweet)\n",
    "    #tweet = re.sub('#\\S+', '', tweet)  # remove hashtags\n",
    "    #tweet = re.sub('@\\S+', '', tweet)  # remove mentions (Make GDPR Compliant)\n",
    "    tweet = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), '', tweet)  # remove punctuations\n",
    "    tweet = re.sub(r\"[a-zA-Z]{1,2}\\d+[a-zA-Z]{1,4}\",r\"tracking_number\", tweet) # replace tracking numbers witht the string 'tracking_number'\n",
    "    tweet = re.sub(\"\\d+\", \"\", tweet) # Remove numbers (but do not remove numbers in words)\n",
    "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet) #Word Lengthening, Remove redundant letters e.g heyyyyyyaaaa to heyyaa\n",
    "    tweet = re.sub('[rR]oyal[mM]ail', '', tweet)  # remove royalmail\n",
    "    \n",
    "    \n",
    "    #Stemming\n",
    "    snowball_stemmer = SnowballStemmer(\"english\")\n",
    "    tweet = snowball_stemmer.stem(tweet)\n",
    "    \n",
    "    #Split text and make into tokens\n",
    "    #tweet = re.split(\"\\s+\",tweet)\n",
    "    \n",
    "    #Use a specific Twitter Tokenizer\n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    T = TweetTokenizer()\n",
    "    tweet = T.tokenize(tweet)\n",
    "    \n",
    "    #All iterables '' that are false (None) will be removed.\n",
    "    tweet = filter(None, tweet)\n",
    "    \n",
    "    #lemitisation\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tweet =[wnl.lemmatize(i) for i in tweet]\n",
    "    \n",
    "\n",
    "    #All word to lowercase\n",
    "    tweet = [t.lower() for t in tweet]\n",
    "    \n",
    "    #Remove stopwords\n",
    "    tweet = [word for word in tweet if word not in stopwordsmodified]\n",
    "\n",
    "    \n",
    "    # Spell correct words\n",
    "    #tweet = correctspelling(tweet) # Accuracy is less when correcting incorrect words\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = []          # the filtered data from the dataset file (should be 20315 samples)\n",
    "preprocessedData = [] # the preprocessed reviews ( to see how the preprocessing is doing)\n",
    "\n",
    "TweetPath = cwd + '/data/cleanedTweets.csv'\n",
    "\n",
    "loadData(TweetPath) # Run the Data Transformation on the cleaned tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens without transformation 450794\n",
      "Number of unique tokens with transformation 20169\n"
     ]
    }
   ],
   "source": [
    "def countNumberOfUniqueWordsRAW(data):\n",
    "    totalTokens=[]\n",
    "    for a in rawData:\n",
    "        totalTokens.append(a[1])\n",
    "    totalWordsRawData = sum(collections.Counter(' '.join(totalTokens).split()).values())\n",
    "    print('Number of unique tokens without transformation', totalWordsRawData)\n",
    "    \n",
    "def countNumberOfUniqueWordsPROCESSED(data):\n",
    "    totalTokens=[]\n",
    "    for a in data:\n",
    "        totalTokens = totalTokens + a[1]\n",
    "    print('Number of unique tokens with transformation', len(set(totalTokens)))\n",
    "    \n",
    "def countUniqueTokens(rawData,preprocessedData):\n",
    "    countNumberOfUniqueWordsRAW(rawData)\n",
    "    countNumberOfUniqueWordsPROCESSED(preprocessedData)\n",
    "    \n",
    "countUniqueTokens(rawData,preprocessedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = cwd +'/data/GDPRcompliantTweets.dump'\n",
    "\n",
    "# open the file for writing\n",
    "fileObject = open(file_name,'wb')\n",
    "\n",
    "# this writes the object preprocessedData to the\n",
    "# file named 'GDPRcompliantTweets.csv'\n",
    "pickle.dump(preprocessedData,fileObject)\n",
    "\n",
    "# here we close the fileObject\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tracking number is tracking_number'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = 'The tracking number is XH8977647697GB'\n",
    "tweet = re.sub(r\"[a-zA-Z]{1,2}\\d+[a-zA-Z]{1,4}\",r\"tracking_number\", tweet) # replace tracking numbers witht the string 'tracking_number'\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check data still looks okay (emoji's are still there, spellings look normal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
